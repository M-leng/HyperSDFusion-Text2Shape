model:
  params:
    linear_start: 0.00085
    linear_end: 0.012
    conditioning_key: crossattn
    timesteps: 1000
    scale_factor: 0.18215

bert:
  params:
    n_embed: 1280
    n_layer: 32

unet:
  target: ldm.modules.diffusionmodules.openaimodel.UNetModel
  params:
    image_size: 16
    in_channels: 3
    out_channels: 3
    model_channels: 224
    num_res_blocks: 2
    attention_resolutions:
    # note: this isn\t actually the resolution but
    # the downsampling factor, i.e. this corresnponds to
    # attention on spatial resolution 8,16,32, as the
    # spatial reolution of the latents is 64 for f4
    - 4
    - 2
    # - 1
    channel_mult:
    - 1
    - 2
    - 4
    # - 4
    # num_head_channels: 32
    num_heads: 8

    # 3d
    dims: 3

    # cond params
    use_spatial_transformer: true
    transformer_depth: 1
    # context_dim: 3072 # 2048 + bert.params.n_embed
    context_dim: 1280 # 2048 + bert.params.n_embed
    use_checkpoint: true
    legacy: False

    # default params
    # dropout: 0
    # conv_resample: True
    # dims: 2
    # num_classes: None





pe:
  mode: 'fourier'
  init_factor: 10
  pos_dim: 3
  zq_dim: 8
  pos_embed_dim: 128

data:
  resolution: 64